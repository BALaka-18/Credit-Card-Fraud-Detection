{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, auc, roc_auc_score, recall_score, precision_score, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284802</td>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284803</td>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284804</td>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284805</td>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284806</td>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\BIPLAB\\\\Desktop\\\\creditcard.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n"
     ]
    }
   ],
   "source": [
    "# shape\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
      "6   4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "5  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398 -0.371427 -0.232794   \n",
      "6  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104 -0.780055  0.750137   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "5  0.105915  0.253844  0.081080    3.67      0  \n",
      "6 -0.257237  0.034507  0.005168    4.99      0  \n",
      "\n",
      "[7 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      "Time      284807 non-null float64\n",
      "V1        284807 non-null float64\n",
      "V2        284807 non-null float64\n",
      "V3        284807 non-null float64\n",
      "V4        284807 non-null float64\n",
      "V5        284807 non-null float64\n",
      "V6        284807 non-null float64\n",
      "V7        284807 non-null float64\n",
      "V8        284807 non-null float64\n",
      "V9        284807 non-null float64\n",
      "V10       284807 non-null float64\n",
      "V11       284807 non-null float64\n",
      "V12       284807 non-null float64\n",
      "V13       284807 non-null float64\n",
      "V14       284807 non-null float64\n",
      "V15       284807 non-null float64\n",
      "V16       284807 non-null float64\n",
      "V17       284807 non-null float64\n",
      "V18       284807 non-null float64\n",
      "V19       284807 non-null float64\n",
      "V20       284807 non-null float64\n",
      "V21       284807 non-null float64\n",
      "V22       284807 non-null float64\n",
      "V23       284807 non-null float64\n",
      "V24       284807 non-null float64\n",
      "V25       284807 non-null float64\n",
      "V26       284807 non-null float64\n",
      "V27       284807 non-null float64\n",
      "V28       284807 non-null float64\n",
      "Amount    284807 non-null float64\n",
      "Class     284807 non-null int64\n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# check null values exist or not\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284803    172787.0\n",
      "284804    172788.0\n",
      "284805    172788.0\n",
      "284806    172792.0\n",
      "Name: Time, dtype: float64\n",
      "284803    47.996389\n",
      "284804    47.996667\n",
      "284805    47.996667\n",
      "284806    47.997778\n",
      "Name: Time_Hr, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['Time'].tail(4))\n",
    "df[\"Time_Hr\"] = df[\"Time\"]/3600\n",
    "print(df['Time_Hr'].tail(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting against time to find a trend\n",
    "plt.hist(df.Time_Hr[df.Class == 0],bins=48,color='g',alpha=0.5)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Transactions\")\n",
    "plt.title(\"Genuine\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df.Time_Hr[df.Class == 1],bins=48,color='r',alpha=0.5)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Transactions\")\n",
    "plt.title(\"Fraud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting against amount to find a trend\n",
    "plt.hist(df.Amount[df.Class == 0],bins = 50,color = 'g',alpha=0.5)\n",
    "plt.xlabel(\"Amount\")\n",
    "plt.ylabel(\"Transactions\")\n",
    "plt.title(\"Fraud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df.Amount[df.Class == 1],bins = 50,color = 'r',alpha=0.5)\n",
    "plt.xlabel(\"Amount\")\n",
    "plt.ylabel(\"Transactions\")\n",
    "plt.title(\"Fraud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>Time_Hr</th>\n",
       "      <th>Scaled_Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284802</td>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0</td>\n",
       "      <td>47.996111</td>\n",
       "      <td>-0.350151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284803</td>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>0</td>\n",
       "      <td>47.996389</td>\n",
       "      <td>-0.254117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284804</td>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>0</td>\n",
       "      <td>47.996667</td>\n",
       "      <td>-0.081839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284805</td>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>0</td>\n",
       "      <td>47.996667</td>\n",
       "      <td>-0.313249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284806</td>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>0</td>\n",
       "      <td>47.997778</td>\n",
       "      <td>0.514355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V22       V23  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ...  0.277838 -0.110474   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.638672  0.101288   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.771679  0.909412   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ...  0.005274 -0.190321   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ...  0.798278 -0.137458   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.111864  1.014480   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.924384  0.012463   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.578229 -0.037501   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.800049 -0.163298   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.643078  0.376777   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Class    Time_Hr  \\\n",
       "0       0.066928  0.128539 -0.189115  0.133558 -0.021053      0   0.000000   \n",
       "1      -0.339846  0.167170  0.125895 -0.008983  0.014724      0   0.000000   \n",
       "2      -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0   0.000278   \n",
       "3      -1.175575  0.647376 -0.221929  0.062723  0.061458      0   0.000278   \n",
       "4       0.141267 -0.206010  0.502292  0.219422  0.215153      0   0.000556   \n",
       "...          ...       ...       ...       ...       ...    ...        ...   \n",
       "284802 -0.509348  1.436807  0.250034  0.943651  0.823731      0  47.996111   \n",
       "284803 -1.016226 -0.606624 -0.395255  0.068472 -0.053527      0  47.996389   \n",
       "284804  0.640134  0.265745 -0.087371  0.004455 -0.026561      0  47.996667   \n",
       "284805  0.123205 -0.569159  0.546668  0.108821  0.104533      0  47.996667   \n",
       "284806  0.008797 -0.473649 -0.818267 -0.002415  0.013649      0  47.997778   \n",
       "\n",
       "        Scaled_Amount  \n",
       "0            0.244964  \n",
       "1           -0.342475  \n",
       "2            1.160686  \n",
       "3            0.140534  \n",
       "4           -0.073403  \n",
       "...               ...  \n",
       "284802      -0.350151  \n",
       "284803      -0.254117  \n",
       "284804      -0.081839  \n",
       "284805      -0.313249  \n",
       "284806       0.514355  \n",
       "\n",
       "[284807 rows x 32 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardizing the Amounts.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df['Scaled_Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df = df.drop(['Amount'], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Class',\n",
      "       'Time_Hr', 'Scaled_Amount'],\n",
      "      dtype='object')\n",
      "\n",
      "Train size :  227845 \n",
      "Test size :  56962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(            Time        V1        V2        V3        V4        V5        V6  \\\n",
       " 159275  112388.0  2.026190 -0.041764 -1.395439  0.067624  0.314498 -0.530806   \n",
       " 244838  152509.0 -0.195504  1.545041  1.031471  2.482815  1.695925  0.434180   \n",
       " 156230  107586.0 -0.105025  1.309503 -0.365174 -0.391590  1.033099 -0.811986   \n",
       " 48726    43759.0 -0.854246  0.701998  1.166225 -1.185470 -0.782259 -0.933049   \n",
       " 144088   85853.0  1.060077  0.093933  0.099762  0.985735 -0.059163 -0.401092   \n",
       " ...          ...       ...       ...       ...       ...       ...       ...   \n",
       " 35691    38224.0  0.954793 -0.177613  1.180126  2.811455 -0.470870  1.165166   \n",
       " 121555   76237.0  1.163033  0.235939  0.413609  1.389495 -0.109340 -0.169348   \n",
       " 51012    44750.0 -0.717907  0.780226  0.845612  0.562767 -0.000699 -1.067275   \n",
       " 160120  113126.0  1.688272 -1.782414 -0.810458 -0.754815 -1.208454 -0.185804   \n",
       " 75836    56275.0 -0.482144  0.471148  2.278648  1.579943 -0.218163  0.270482   \n",
       " \n",
       "               V7        V8        V9  ...       V21       V22       V23  \\\n",
       " 159275  0.089613 -0.159198  0.396139  ...  0.332471  1.105277  0.007347   \n",
       " 244838  1.519682 -0.262392 -1.870362  ... -0.229081 -0.542161 -0.025016   \n",
       " 156230  0.874705 -0.203432  1.362339  ... -0.517254 -1.010887  0.116913   \n",
       " 48726   0.103878  0.565584 -0.198935  ...  0.013683 -0.208008  0.134779   \n",
       " 144088  0.279366 -0.135144 -0.392356  ...  0.118886  0.201846 -0.238252   \n",
       " ...          ...       ...       ...  ...       ...       ...       ...   \n",
       " 35691  -0.589297  0.350384  0.474962  ...  0.063350  0.408192 -0.230346   \n",
       " 121555  0.099044 -0.040071  0.224988  ... -0.096174 -0.021381 -0.068110   \n",
       " 51012   0.432641 -0.159388  0.018698  ... -0.243674 -0.587405  0.429360   \n",
       " 160120 -0.818740 -0.082303  0.073899  ...  0.542459  1.026461 -0.059813   \n",
       " 75836   0.165015  0.135637  0.389834  ... -0.271196 -0.337419  0.058334   \n",
       " \n",
       "              V24       V25       V26       V27       V28    Time_Hr  \\\n",
       " 159275  0.744880  0.320343 -0.471884  0.006951 -0.055800  31.218889   \n",
       " 244838  0.424075 -0.512201 -0.477338 -0.024537 -0.027638  42.363611   \n",
       " 156230  0.507027 -0.370831  0.078228  0.300587  0.148577  29.885000   \n",
       " 48726   0.544740 -0.307539  0.696862  0.029857 -0.010955  12.155278   \n",
       " 144088  0.037650  0.699182 -0.337154 -0.001166  0.023533  23.848056   \n",
       " ...          ...       ...       ...       ...       ...        ...   \n",
       " 35691  -0.377894  0.583491  0.265740  0.050216  0.032017  10.617778   \n",
       " 121555  0.106814  0.677528 -0.314379  0.042516  0.017160  21.176944   \n",
       " 51012   0.307083 -0.549398  0.088874 -0.101395  0.163409  12.430556   \n",
       " 160120  0.656704 -0.268181 -0.116382 -0.031961  0.004533  31.423889   \n",
       " 75836   0.430530 -0.375968 -0.550834  0.231963  0.183458  15.631944   \n",
       " \n",
       "         Scaled_Amount  \n",
       " 159275      -0.349231  \n",
       " 244838      -0.309210  \n",
       " 156230      -0.342475  \n",
       " 48726       -0.199623  \n",
       " 144088       0.006598  \n",
       " ...               ...  \n",
       " 35691       -0.041818  \n",
       " 121555      -0.315407  \n",
       " 51012       -0.346073  \n",
       " 160120       0.692790  \n",
       " 75836       -0.245001  \n",
       " \n",
       " [227845 rows x 31 columns],\n",
       "             Time        V1        V2        V3        V4        V5        V6  \\\n",
       " 32391    36777.0 -0.434497  0.267650  1.477930 -1.064791 -0.059986 -0.223504   \n",
       " 184141  126111.0  2.092116 -1.734571 -1.247354 -1.826390 -0.583705  1.068898   \n",
       " 9750     14483.0  1.180524 -0.006209  0.992223  0.349863 -0.768270 -0.547693   \n",
       " 156255  107674.0  2.348553 -1.280062 -1.198796 -1.492207 -0.799846 -0.377914   \n",
       " 226331  144586.0 -0.280252  1.163228 -0.081300 -0.566846  0.165346 -1.136529   \n",
       " ...          ...       ...       ...       ...       ...       ...       ...   \n",
       " 278500  168252.0  2.071222 -1.855814  0.344391 -1.378738 -2.256678 -0.232811   \n",
       " 160601  113486.0  2.055797 -0.326668 -2.752041 -0.842316  2.463072  3.173856   \n",
       " 255808  157410.0 -3.332140 -1.470814 -1.026332  0.113620 -0.161671  0.007031   \n",
       " 135132   81120.0  1.202347  0.177485  0.611566  0.556860 -0.555679 -0.793957   \n",
       " 273931  165771.0 -0.731239  1.902541  1.790264  4.120062  0.267796  1.463860   \n",
       " \n",
       "               V7        V8        V9  ...       V21       V22       V23  \\\n",
       " 32391   1.062713 -0.245124  0.055777  ... -0.124518 -0.455414  0.152332   \n",
       " 184141 -1.394017  0.341750 -1.290420  ...  0.032335  0.591249  0.140917   \n",
       " 9750   -0.476450 -0.111406  1.643402  ... -0.012229  0.280578  0.048929   \n",
       " 156255 -1.118134 -0.287223  0.167890  ... -0.379921 -0.340680  0.113740   \n",
       " 226331  0.674048  0.200109 -0.272361  ... -0.219536 -0.557600  0.105981   \n",
       " ...          ...       ...       ...  ...       ...       ...       ...   \n",
       " 278500 -1.894637 -0.007581 -0.613509  ...  0.130433  0.845139  0.203987   \n",
       " 160601 -0.432126  0.727706  0.608606  ...  0.269765  0.844627  0.020675   \n",
       " 255808  0.590399  0.794440 -0.208451  ...  0.237433  0.117783 -0.427271   \n",
       " 135132 -0.061710 -0.050472 -0.172218  ... -0.196478 -0.624949  0.153605   \n",
       " 273931 -0.157464  0.754943 -1.944788  ... -0.249949 -0.663957 -0.220804   \n",
       " \n",
       "              V24       V25       V26       V27       V28    Time_Hr  \\\n",
       " 32391  -0.441208 -0.573574  0.620385 -0.153927 -0.086575  10.215833   \n",
       " 184141 -1.651072 -0.306022  0.044429  0.044879 -0.073752  35.030833   \n",
       " 9750    0.429270  0.105318  1.039268 -0.069574  0.006653   4.023056   \n",
       " 156255 -1.149295 -0.081060 -0.131328 -0.019994 -0.067867  29.909444   \n",
       " 226331 -0.020018 -0.443274  0.143283  0.125409  0.035808  40.162778   \n",
       " ...          ...       ...       ...       ...       ...        ...   \n",
       " 278500  0.014965 -0.534224 -0.099805  0.078721 -0.011173  46.736667   \n",
       " 160601  0.726212  0.366624 -0.398828  0.027735 -0.060282  31.523889   \n",
       " 255808 -0.957036  0.088677 -0.250888  0.425731 -0.284736  43.725000   \n",
       " 135132  0.520154  0.147388  0.065695 -0.035080  0.010145  22.533333   \n",
       " 273931 -1.107417  0.079056  0.278438  0.245959  0.091996  46.047500   \n",
       " \n",
       "         Scaled_Amount  \n",
       " 32391        0.113547  \n",
       " 184141      -0.130376  \n",
       " 9750        -0.289460  \n",
       " 156255      -0.253277  \n",
       " 226331      -0.349271  \n",
       " ...               ...  \n",
       " 278500      -0.009394  \n",
       " 160601      -0.349231  \n",
       " 255808       0.983691  \n",
       " 135132      -0.345313  \n",
       " 273931      -0.349231  \n",
       " \n",
       " [56962 rows x 31 columns],\n",
       " 159275    0\n",
       " 244838    0\n",
       " 156230    0\n",
       " 48726     0\n",
       " 144088    0\n",
       "          ..\n",
       " 35691     0\n",
       " 121555    0\n",
       " 51012     0\n",
       " 160120    0\n",
       " 75836     0\n",
       " Name: Class, Length: 227845, dtype: int64,\n",
       " 32391     0\n",
       " 184141    0\n",
       " 9750      0\n",
       " 156255    0\n",
       " 226331    0\n",
       "          ..\n",
       " 278500    0\n",
       " 160601    0\n",
       " 255808    0\n",
       " 135132    0\n",
       " 273931    0\n",
       " Name: Class, Length: 56962, dtype: int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split(df, dropped):\n",
    "    df = df.drop(dropped, axis = 1)\n",
    "    print(df.columns)\n",
    "    # Train Test splitting\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    y = df['Class']\n",
    "    x = df.drop(['Class'], axis = 1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 40, stratify = y)\n",
    "    \n",
    "    print(\"\\nTrain size : \", len(y_train), \"\\nTest size : \", len(y_test))\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "# Check\n",
    "drop = []\n",
    "split(df, drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(classifier, x_train, y_train, x_test):\n",
    "    classifier = classifier   #Creating the classifier\n",
    "    classifier.fit(x_train, y_train)\n",
    "    predict = classifier.predict(x_test)\n",
    "    #Predicted probabilities\n",
    "    prob = classifier.predict_proba(x_test)\n",
    "    return predict, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(y_test, predict, prob):\n",
    "    print(\"\\n1. Confusion matrix : \", confusion_matrix(y_test, predict))\n",
    "    print(\"\\n2. Recall score : \", recall_score(y_test, predict))\n",
    "    print(\"\\n3. Accuracy score : \", accuracy_score(y_test, predict))\n",
    "    print(\"\\n4. Precision score : \", precision_score(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Class',\n",
      "       'Time_Hr', 'Scaled_Amount'],\n",
      "      dtype='object')\n",
      "\n",
      "Train size :  227845 \n",
      "Test size :  56962\n",
      "\n",
      " [0 0 0 ... 0 0 0] [[1.00000000e+00 8.00011081e-14]\n",
      " [1.00000000e+00 1.17212255e-13]\n",
      " [1.00000000e+00 5.66750767e-13]\n",
      " ...\n",
      " [1.00000000e+00 8.40884703e-14]\n",
      " [1.00000000e+00 1.43758805e-13]\n",
      " [1.00000000e+00 1.39275158e-12]]\n",
      "\n",
      "1. Confusion matrix :  [[56544   320]\n",
      " [   44    54]]\n",
      "\n",
      "2. Recall score :  0.5510204081632653\n",
      "\n",
      "3. Accuracy score :  0.9936097749376778\n",
      "\n",
      "4. Precision score :  0.1443850267379679\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Doing the actual prediction with Gaussian Naive Bayes\n",
    "\n",
    "#Case 1 : Dropping none of the columns\n",
    "dropped = []\n",
    "X_train, X_test, Y_train, Y_test = split(df, dropped)\n",
    "y_pred, y_prob = predictions(GaussianNB(), X_train, Y_train, X_test)\n",
    "print(\"\\n\",y_pred, y_prob)\n",
    "scores(Y_test, y_pred, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V7', 'V8', 'V12', 'V13', 'V14',\n",
      "       'V15', 'V19', 'V21', 'V22', 'V23', 'V24', 'V26', 'Class', 'Time_Hr',\n",
      "       'Scaled_Amount'],\n",
      "      dtype='object')\n",
      "\n",
      "Train size :  227845 \n",
      "Test size :  56962\n",
      "\n",
      " [0 0 0 ... 0 0 0] [[9.99999999e-01 6.07427451e-10]\n",
      " [1.00000000e+00 3.02505906e-10]\n",
      " [9.99999998e-01 2.11810234e-09]\n",
      " ...\n",
      " [9.99999999e-01 5.23807259e-10]\n",
      " [1.00000000e+00 3.46822159e-10]\n",
      " [9.99999988e-01 1.16500977e-08]]\n",
      "\n",
      "1. Confusion matrix :  [[56468   396]\n",
      " [   47    51]]\n",
      "\n",
      "2. Recall score :  0.5204081632653061\n",
      "\n",
      "3. Accuracy score :  0.9922228854323936\n",
      "\n",
      "4. Precision score :  0.11409395973154363\n"
     ]
    }
   ],
   "source": [
    "#Case 2 : We are dropping some important parameters of the PCA components\n",
    "dropped = ['V6', 'V28', 'V27', 'V16', 'V18', 'V9', 'V10', 'V25', 'V20', 'V17', 'V11']\n",
    "X_train, X_test, Y_train, Y_test = split(df, dropped)\n",
    "y_pred, y_prob = predictions(GaussianNB(), X_train, Y_train, X_test)\n",
    "print(\"\\n\",y_pred, y_prob)\n",
    "scores(Y_test, y_pred, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V7', 'V12', 'V13', 'V14', 'V15', 'V19',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V26', 'Class', 'Scaled_Amount'],\n",
      "      dtype='object')\n",
      "\n",
      "Train size :  227845 \n",
      "Test size :  56962\n",
      "\n",
      " [0 0 0 ... 0 0 0] [[9.99999994e-01 5.50918021e-09]\n",
      " [9.99999996e-01 4.22507608e-09]\n",
      " [9.99999985e-01 1.46682824e-08]\n",
      " ...\n",
      " [9.99999991e-01 8.79670568e-09]\n",
      " [9.99999996e-01 3.81166153e-09]\n",
      " [9.99999785e-01 2.15151012e-07]]\n",
      "\n",
      "1. Confusion matrix :  [[56571   293]\n",
      " [   45    53]]\n",
      "\n",
      "2. Recall score :  0.5408163265306123\n",
      "\n",
      "3. Accuracy score :  0.9940662195849865\n",
      "\n",
      "4. Precision score :  0.1531791907514451\n"
     ]
    }
   ],
   "source": [
    "#Case 3 : We are dropping some important parameters of the PCA components and Time too\n",
    "dropped = ['Time_Hr', 'V6', 'V28', 'V27', 'V16', 'V18', 'V9', 'V10', 'V25', 'V20', 'V17', 'V11', 'V8', 'V5']\n",
    "X_train, X_test, Y_train, Y_test = split(df, dropped)\n",
    "y_pred, y_prob = predictions(GaussianNB(), X_train, Y_train, X_test)\n",
    "print(\"\\n\",y_pred, y_prob)\n",
    "scores(Y_test, y_pred, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V7', 'V8', 'V12', 'V13', 'V14',\n",
      "       'V15', 'V19', 'V21', 'V22', 'V23', 'V24', 'V26', 'V28', 'Class'],\n",
      "      dtype='object')\n",
      "\n",
      "Train size :  227845 \n",
      "Test size :  56962\n",
      "\n",
      " [0 0 0 ... 0 0 0] [[1.00000000e+00 4.10211063e-10]\n",
      " [1.00000000e+00 3.51196080e-10]\n",
      " [9.99999999e-01 1.26487187e-09]\n",
      " ...\n",
      " [9.99999999e-01 7.06393382e-10]\n",
      " [1.00000000e+00 3.12586097e-10]\n",
      " [9.99999983e-01 1.71622036e-08]]\n",
      "\n",
      "1. Confusion matrix :  [[56464   400]\n",
      " [   47    51]]\n",
      "\n",
      "2. Recall score :  0.5204081632653061\n",
      "\n",
      "3. Accuracy score :  0.9921526631789614\n",
      "\n",
      "4. Precision score :  0.1130820399113082\n"
     ]
    }
   ],
   "source": [
    "#Case 4 : We are dropping some important parameters of the PCA components + Time + Scaled_Amount\n",
    "dropped = ['Time_Hr', 'Scaled_Amount', 'V6', 'V27', 'V16', 'V18', 'V9', 'V10', 'V25', 'V20', 'V17', 'V11']\n",
    "X_train, X_test, Y_train, Y_test = split(df, dropped)\n",
    "y_pred, y_prob = predictions(GaussianNB(), X_train, Y_train, X_test)\n",
    "print(\"\\n\",y_pred, y_prob)\n",
    "scores(Y_test, y_pred, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Class',\n",
      "       'Time_Hr', 'Scaled_Amount'],\n",
      "      dtype='object')\n",
      "\n",
      "Train size :  227845 \n",
      "Test size :  56962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda 3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [0 0 0 ... 0 0 0] [[9.90087467e-01 9.91253302e-03]\n",
      " [9.99979912e-01 2.00880157e-05]\n",
      " [9.74647001e-01 2.53529995e-02]\n",
      " ...\n",
      " [9.99998473e-01 1.52669312e-06]\n",
      " [9.99680064e-01 3.19936392e-04]\n",
      " [9.99999512e-01 4.87726767e-07]]\n",
      "\n",
      "1. Confusion matrix :  [[56847    17]\n",
      " [   68    30]]\n",
      "\n",
      "2. Recall score :  0.30612244897959184\n",
      "\n",
      "3. Accuracy score :  0.9985077771145676\n",
      "\n",
      "4. Precision score :  0.6382978723404256\n"
     ]
    }
   ],
   "source": [
    "#Doing the actual prediction with Logistic Regression\n",
    "#Case 1 : Dropping none of the columns\n",
    "dropd = []\n",
    "X_train, X_test, Y_train, Y_test = split(df, dropd)\n",
    "y_pred, y_prob = predictions(LogisticRegression(penalty = 'l1', C = 0.001), X_train, Y_train, X_test)\n",
    "print(\"\\n\",y_pred, y_prob)\n",
    "scores(Y_test, y_pred, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V7', 'V13', 'V14', 'V15', 'V19',\n",
      "       'V23', 'V24', 'V26', 'Class', 'Time_Hr', 'Scaled_Amount'],\n",
      "      dtype='object')\n",
      "\n",
      "Train size :  227845 \n",
      "Test size :  56962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda 3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [0 0 0 ... 0 0 0] [[9.90913687e-01 9.08631344e-03]\n",
      " [9.99977869e-01 2.21309960e-05]\n",
      " [9.74996540e-01 2.50034598e-02]\n",
      " ...\n",
      " [9.99998545e-01 1.45511462e-06]\n",
      " [9.99699383e-01 3.00616688e-04]\n",
      " [9.99999510e-01 4.90145806e-07]]\n",
      "\n",
      "1. Confusion matrix :  [[56839    25]\n",
      " [   68    30]]\n",
      "\n",
      "2. Recall score :  0.30612244897959184\n",
      "\n",
      "3. Accuracy score :  0.9983673326077034\n",
      "\n",
      "4. Precision score :  0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "#Case 2 : We are dropping some important parameters of the PCA components\n",
    "dropd = ['V6', 'V28', 'V27', 'V16', 'V18', 'V9', 'V10', 'V25', 'V20', 'V17', 'V11', 'V8', 'V12', 'V21', 'V22']\n",
    "X_train, X_test, Y_train, Y_test = split(df, dropd)\n",
    "y_pred, y_prob = predictions(LogisticRegression(penalty = 'l1', C = 0.001), X_train, Y_train, X_test)\n",
    "print(\"\\n\",y_pred, y_prob)\n",
    "scores(Y_test, y_pred, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V7', 'V12', 'V13', 'V14', 'V15', 'V19',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V26', 'Class', 'Scaled_Amount'],\n",
      "      dtype='object')\n",
      "\n",
      "Train size :  227845 \n",
      "Test size :  56962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda 3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [0 0 0 ... 0 0 0] [[9.90913247e-01 9.08675255e-03]\n",
      " [9.99977948e-01 2.20517326e-05]\n",
      " [9.74983178e-01 2.50168222e-02]\n",
      " ...\n",
      " [9.99998552e-01 1.44777155e-06]\n",
      " [9.99699964e-01 3.00035782e-04]\n",
      " [9.99999512e-01 4.87547090e-07]]\n",
      "\n",
      "1. Confusion matrix :  [[56839    25]\n",
      " [   68    30]]\n",
      "\n",
      "2. Recall score :  0.30612244897959184\n",
      "\n",
      "3. Accuracy score :  0.9983673326077034\n",
      "\n",
      "4. Precision score :  0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "#Case 3 : We are dropping some important parameters of the PCA components + Time + Scaled_Amount\n",
    "dropd = ['Time_Hr', 'V6', 'V28', 'V27', 'V16', 'V18', 'V9', 'V10', 'V25', 'V20', 'V17', 'V11', 'V8', 'V5']\n",
    "X_train, X_test, Y_train, Y_test = split(df, dropd)\n",
    "y_pred, y_prob = predictions(LogisticRegression(penalty = 'l1', C = 0.001), X_train, Y_train, X_test)\n",
    "print(\"\\n\",y_pred, y_prob)\n",
    "scores(Y_test, y_pred, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V7', 'V12', 'V13', 'V14', 'V15', 'V19',\n",
      "       'V23', 'V24', 'V26', 'V28', 'Class'],\n",
      "      dtype='object')\n",
      "\n",
      "Train size :  227845 \n",
      "Test size :  56962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda 3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [0 0 0 ... 0 0 0] [[9.90912646e-01 9.08735377e-03]\n",
      " [9.99977883e-01 2.21172929e-05]\n",
      " [9.74996423e-01 2.50035768e-02]\n",
      " ...\n",
      " [9.99998546e-01 1.45387434e-06]\n",
      " [9.99699468e-01 3.00531754e-04]\n",
      " [9.99999510e-01 4.89856323e-07]]\n",
      "\n",
      "1. Confusion matrix :  [[56839    25]\n",
      " [   68    30]]\n",
      "\n",
      "2. Recall score :  0.30612244897959184\n",
      "\n",
      "3. Accuracy score :  0.9983673326077034\n",
      "\n",
      "4. Precision score :  0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "#Case 4 : We are dropping some important parameters of the PCA components + Time + Scaled_Amount\n",
    "dropd = ['Time_Hr', 'Scaled_Amount', 'V6', 'V27', 'V16', 'V18', 'V9', 'V10', 'V25', 'V20', 'V17', 'V11', 'V5', 'V8', 'V21', 'V22']\n",
    "X_train, X_test, Y_train, Y_test = split(df, dropd)\n",
    "y_pred, y_prob = predictions(LogisticRegression(penalty = 'l1', C = 0.001), X_train, Y_train, X_test)\n",
    "print(\"\\n\",y_pred, y_prob)\n",
    "scores(Y_test, y_pred, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transactions in undersampled data :  984\n",
      "\n",
      "Percetage genuine transactions :  50 %\n",
      "\n",
      "Percetage fraud transactions :  50 %\n"
     ]
    }
   ],
   "source": [
    "# We see performance on imbalanced dataset is poor\n",
    "# So we train the model on 50/50 under-sampled data, i.e, we take 50/50 ratio of both the classes\n",
    "\n",
    "# Step 1: Get indices of fraud and genuine data\n",
    "f_ind = np.array(df[df.Class == 1].index)\n",
    "g_ind = df[df.Class == 0].index\n",
    "\n",
    "# Total number of fraud cases\n",
    "fraud = len(df[df.Class == 1])\n",
    "\n",
    "# Step 2: Select randomly from genuine class\n",
    "rand_gen = np.array(np.random.choice(g_ind, fraud, replace = False))\n",
    "\n",
    "# Step 3: Merging the two class indices : random genuine + original fraud\n",
    "under_sample = np.concatenate([f_ind, rand_gen])\n",
    "\n",
    "# Step 4: Creating the undersampled dataset and separating features and target data\n",
    "new_df = df.iloc[under_sample,:]     # Creating the under sample dataset\n",
    "y_df = new_df['Class'].values        # Label/Target\n",
    "x_df = new_df.drop(['Class'], axis = 1).values      # Features\n",
    "\n",
    "# Step 5: Some information extraction\n",
    "print(\"\\nTransactions in undersampled data : \", len(new_df))\n",
    "print(\"\\nPercetage genuine transactions : \", int((len(new_df[new_df.Class == 0])/len(new_df))*100), \"%\")\n",
    "print(\"\\nPercetage fraud transactions : \", int((len(new_df[new_df.Class == 1])/len(new_df))*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Class',\n",
      "       'Time_Hr', 'Scaled_Amount'],\n",
      "      dtype='object')\n",
      "\n",
      "Train size :  787 \n",
      "Test size :  197\n",
      "\n",
      " [0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0\n",
      " 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1\n",
      " 1 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1\n",
      " 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1\n",
      " 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0\n",
      " 1 1 0 0 0 1 0 1 1 0 1 0] [[7.11781221e-01 2.88218779e-01]\n",
      " [5.62362470e-03 9.94376375e-01]\n",
      " [7.28694654e-01 2.71305346e-01]\n",
      " [1.22679044e-01 8.77320956e-01]\n",
      " [6.50735038e-01 3.49264962e-01]\n",
      " [6.74774100e-01 3.25225900e-01]\n",
      " [5.78317603e-01 4.21682397e-01]\n",
      " [7.96562520e-01 2.03437480e-01]\n",
      " [7.63126742e-01 2.36873258e-01]\n",
      " [2.28431541e-01 7.71568459e-01]\n",
      " [9.18424691e-01 8.15753088e-02]\n",
      " [8.34247166e-01 1.65752834e-01]\n",
      " [2.66782757e-03 9.97332172e-01]\n",
      " [7.02876661e-04 9.99297123e-01]\n",
      " [9.45523730e-01 5.44762696e-02]\n",
      " [7.57826067e-01 2.42173933e-01]\n",
      " [1.57747836e-01 8.42252164e-01]\n",
      " [6.78910861e-01 3.21089139e-01]\n",
      " [3.59207734e-01 6.40792266e-01]\n",
      " [4.85905715e-01 5.14094285e-01]\n",
      " [6.71088949e-01 3.28911051e-01]\n",
      " [8.71202534e-01 1.28797466e-01]\n",
      " [8.45490507e-06 9.99991545e-01]\n",
      " [6.12002883e-02 9.38799712e-01]\n",
      " [6.92198230e-01 3.07801770e-01]\n",
      " [6.67160664e-01 3.32839336e-01]\n",
      " [8.99424651e-06 9.99991006e-01]\n",
      " [4.07752854e-04 9.99592247e-01]\n",
      " [2.59908206e-01 7.40091794e-01]\n",
      " [6.62215163e-01 3.37784837e-01]\n",
      " [9.21629098e-01 7.83709025e-02]\n",
      " [6.51533464e-01 3.48466536e-01]\n",
      " [4.26209899e-02 9.57379010e-01]\n",
      " [7.56755462e-01 2.43244538e-01]\n",
      " [9.19344268e-01 8.06557319e-02]\n",
      " [3.55821230e-01 6.44178770e-01]\n",
      " [9.13620292e-01 8.63797075e-02]\n",
      " [9.01364894e-01 9.86351061e-02]\n",
      " [1.04054715e-03 9.98959453e-01]\n",
      " [9.56690037e-01 4.33099631e-02]\n",
      " [8.43542400e-04 9.99156458e-01]\n",
      " [9.46973024e-01 5.30269757e-02]\n",
      " [8.16516668e-01 1.83483332e-01]\n",
      " [3.59087037e-01 6.40912963e-01]\n",
      " [8.08022980e-01 1.91977020e-01]\n",
      " [9.20806692e-01 7.91933078e-02]\n",
      " [8.19496761e-01 1.80503239e-01]\n",
      " [1.97344060e-01 8.02655940e-01]\n",
      " [7.94188837e-01 2.05811163e-01]\n",
      " [1.13413570e-01 8.86586430e-01]\n",
      " [6.68447180e-01 3.31552820e-01]\n",
      " [1.97141865e-01 8.02858135e-01]\n",
      " [1.96905416e-01 8.03094584e-01]\n",
      " [8.06616655e-01 1.93383345e-01]\n",
      " [6.50604381e-01 3.49395619e-01]\n",
      " [7.28558870e-01 2.71441130e-01]\n",
      " [6.69119428e-02 9.33088057e-01]\n",
      " [4.74474055e-01 5.25525945e-01]\n",
      " [8.62771077e-01 1.37228923e-01]\n",
      " [3.73805570e-03 9.96261944e-01]\n",
      " [9.61904685e-03 9.90380953e-01]\n",
      " [4.95291215e-01 5.04708785e-01]\n",
      " [7.22328825e-01 2.77671175e-01]\n",
      " [3.26973908e-01 6.73026092e-01]\n",
      " [1.60640345e-01 8.39359655e-01]\n",
      " [8.76481520e-01 1.23518480e-01]\n",
      " [1.08475319e-02 9.89152468e-01]\n",
      " [9.05902046e-01 9.40979544e-02]\n",
      " [2.77702369e-01 7.22297631e-01]\n",
      " [8.92909169e-02 9.10709083e-01]\n",
      " [7.30181553e-01 2.69818447e-01]\n",
      " [4.67266598e-02 9.53273340e-01]\n",
      " [1.05976289e-01 8.94023711e-01]\n",
      " [9.18125053e-04 9.99081875e-01]\n",
      " [1.21886330e-03 9.98781137e-01]\n",
      " [8.92357186e-05 9.99910764e-01]\n",
      " [9.49653160e-02 9.05034684e-01]\n",
      " [9.26688271e-01 7.33117293e-02]\n",
      " [1.06504846e-03 9.98934952e-01]\n",
      " [9.02432703e-01 9.75672971e-02]\n",
      " [7.51791361e-01 2.48208639e-01]\n",
      " [7.62266944e-03 9.92377331e-01]\n",
      " [5.94613561e-01 4.05386439e-01]\n",
      " [6.15024268e-01 3.84975732e-01]\n",
      " [6.67189286e-03 9.93328107e-01]\n",
      " [2.15723107e-02 9.78427689e-01]\n",
      " [6.79150878e-01 3.20849122e-01]\n",
      " [5.68362380e-01 4.31637620e-01]\n",
      " [4.50160180e-01 5.49839820e-01]\n",
      " [6.97955707e-03 9.93020443e-01]\n",
      " [7.33679866e-04 9.99266320e-01]\n",
      " [9.48508884e-01 5.14911164e-02]\n",
      " [7.88540276e-01 2.11459724e-01]\n",
      " [8.38656630e-01 1.61343370e-01]\n",
      " [7.50235684e-01 2.49764316e-01]\n",
      " [5.22041524e-02 9.47795848e-01]\n",
      " [1.39776989e-01 8.60223011e-01]\n",
      " [6.95988879e-01 3.04011121e-01]\n",
      " [8.91797667e-01 1.08202333e-01]\n",
      " [1.62528464e-01 8.37471536e-01]\n",
      " [5.09762388e-04 9.99490238e-01]\n",
      " [8.62918323e-01 1.37081677e-01]\n",
      " [8.60120647e-01 1.39879353e-01]\n",
      " [8.74672210e-01 1.25327790e-01]\n",
      " [8.47264520e-01 1.52735480e-01]\n",
      " [8.04696482e-01 1.95303518e-01]\n",
      " [9.03163326e-01 9.68366738e-02]\n",
      " [7.35004841e-04 9.99264995e-01]\n",
      " [9.03922514e-01 9.60774860e-02]\n",
      " [4.61450985e-02 9.53854902e-01]\n",
      " [1.41743291e-04 9.99858257e-01]\n",
      " [3.54700634e-02 9.64529937e-01]\n",
      " [7.14151652e-01 2.85848348e-01]\n",
      " [6.03961752e-01 3.96038248e-01]\n",
      " [7.67419054e-01 2.32580946e-01]\n",
      " [4.53995867e-02 9.54600413e-01]\n",
      " [9.45953947e-01 5.40460532e-02]\n",
      " [5.61194987e-01 4.38805013e-01]\n",
      " [5.62362470e-03 9.94376375e-01]\n",
      " [6.44612266e-01 3.55387734e-01]\n",
      " [1.07398441e-02 9.89260156e-01]\n",
      " [9.62571852e-06 9.99990374e-01]\n",
      " [4.39664300e-02 9.56033570e-01]\n",
      " [7.80578193e-01 2.19421807e-01]\n",
      " [9.49137622e-06 9.99990509e-01]\n",
      " [6.88298050e-02 9.31170195e-01]\n",
      " [4.18484511e-01 5.81515489e-01]\n",
      " [5.56777714e-03 9.94432223e-01]\n",
      " [3.24388036e-02 9.67561196e-01]\n",
      " [7.23875379e-01 2.76124621e-01]\n",
      " [9.35227785e-01 6.47722146e-02]\n",
      " [2.86430001e-02 9.71357000e-01]\n",
      " [8.34941764e-01 1.65058236e-01]\n",
      " [7.65843543e-04 9.99234156e-01]\n",
      " [1.03492756e-01 8.96507244e-01]\n",
      " [9.29491284e-01 7.05087160e-02]\n",
      " [4.46352541e-04 9.99553647e-01]\n",
      " [4.83916003e-01 5.16083997e-01]\n",
      " [7.33309219e-01 2.66690781e-01]\n",
      " [8.61913155e-01 1.38086845e-01]\n",
      " [8.63001396e-04 9.99136999e-01]\n",
      " [6.12595981e-01 3.87404019e-01]\n",
      " [6.61878516e-01 3.38121484e-01]\n",
      " [6.95343977e-04 9.99304656e-01]\n",
      " [7.34597180e-01 2.65402820e-01]\n",
      " [3.56486218e-02 9.64351378e-01]\n",
      " [6.94068873e-06 9.99993059e-01]\n",
      " [6.50505356e-06 9.99993495e-01]\n",
      " [9.12896258e-01 8.71037415e-02]\n",
      " [7.49454831e-01 2.50545169e-01]\n",
      " [9.18890511e-01 8.11094885e-02]\n",
      " [4.58496587e-04 9.99541503e-01]\n",
      " [9.20097998e-01 7.99020020e-02]\n",
      " [7.93613773e-01 2.06386227e-01]\n",
      " [7.16891286e-01 2.83108714e-01]\n",
      " [7.88820994e-01 2.11179006e-01]\n",
      " [7.88091889e-01 2.11908111e-01]\n",
      " [6.90025239e-01 3.09974761e-01]\n",
      " [2.12466770e-01 7.87533230e-01]\n",
      " [2.29543763e-03 9.97704562e-01]\n",
      " [8.70008753e-03 9.91299912e-01]\n",
      " [7.06156097e-03 9.92938439e-01]\n",
      " [9.04968772e-01 9.50312279e-02]\n",
      " [8.03391378e-01 1.96608622e-01]\n",
      " [8.78922645e-01 1.21077355e-01]\n",
      " [2.69651942e-02 9.73034806e-01]\n",
      " [3.93356912e-03 9.96066431e-01]\n",
      " [6.65867353e-01 3.34132647e-01]\n",
      " [2.37077671e-01 7.62922329e-01]\n",
      " [8.51209161e-01 1.48790839e-01]\n",
      " [6.49196216e-01 3.50803784e-01]\n",
      " [6.99862418e-01 3.00137582e-01]\n",
      " [7.28760353e-01 2.71239647e-01]\n",
      " [7.82809446e-01 2.17190554e-01]\n",
      " [9.42902732e-01 5.70972685e-02]\n",
      " [6.65237162e-04 9.99334763e-01]\n",
      " [8.46644716e-03 9.91533553e-01]\n",
      " [7.85586850e-01 2.14413150e-01]\n",
      " [7.24717090e-02 9.27528291e-01]\n",
      " [8.35669824e-01 1.64330176e-01]\n",
      " [7.16505620e-01 2.83494380e-01]\n",
      " [7.76484627e-01 2.23515373e-01]\n",
      " [4.46996197e-01 5.53003803e-01]\n",
      " [9.03990089e-01 9.60099115e-02]\n",
      " [6.52193674e-01 3.47806326e-01]\n",
      " [1.22155834e-02 9.87784417e-01]\n",
      " [3.07738801e-01 6.92261199e-01]\n",
      " [9.47795324e-01 5.22046762e-02]\n",
      " [7.33994845e-01 2.66005155e-01]\n",
      " [7.32782563e-01 2.67217437e-01]\n",
      " [4.36049456e-01 5.63950544e-01]\n",
      " [9.16910748e-01 8.30892517e-02]\n",
      " [3.65458864e-01 6.34541136e-01]\n",
      " [1.62156127e-01 8.37843873e-01]\n",
      " [9.18187605e-01 8.18123952e-02]\n",
      " [2.20392199e-01 7.79607801e-01]\n",
      " [7.12365908e-01 2.87634092e-01]]\n",
      "\n",
      "1. Confusion matrix :  [[94  5]\n",
      " [12 86]]\n",
      "\n",
      "2. Recall score : "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda 3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.8775510204081632\n",
      "\n",
      "3. Accuracy score :  0.9137055837563451\n",
      "\n",
      "4. Precision score :  0.945054945054945\n"
     ]
    }
   ],
   "source": [
    "# Now we train the Logistic Regression model on the undersampled dataset\n",
    "\n",
    "drpd = []\n",
    "X_train_new, X_test_new, Y_train_new, Y_test_new = split(new_df, drpd)\n",
    "y_pred_new, y_pred_prob_new = predictions(LogisticRegression(C = 0.01, penalty = 'l1'), X_train_new, Y_train_new, X_test_new)\n",
    "print(\"\\n\", y_pred_new, y_pred_prob_new)\n",
    "scores(Y_test_new, y_pred_new, y_pred_prob_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda 3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix : \n",
      " [[279614   4701]\n",
      " [    88    404]]\n",
      "\n",
      "Precision score :  0.07913809990205681\n",
      "\n",
      "Accuracy score :  0.9831851042987005\n"
     ]
    }
   ],
   "source": [
    "# Now let us train on the undersampled dataset and test on the entire dataset\n",
    "Y_total = df['Class'].values     # Label/Target\n",
    "X_total = df.drop(['Class'], axis = 1).values     # Features\n",
    "\n",
    "logr = LogisticRegression(C = 0.002, penalty = 'l1')\n",
    "logr.fit(x_df, y_df)\n",
    "p = logr.predict(X_total)\n",
    "\n",
    "print(\"\\nConfusion matrix : \\n\", confusion_matrix(Y_total, p)) \n",
    "print(\"\\nPrecision score : \", precision_score(Y_total, p))\n",
    "print(\"\\nAccuracy score : \", accuracy_score(Y_total, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hence we see that Logistic Regression gives better model sensitivity, whereas predictive value(positive) is more for \\n   Gaussian Naive Bayes. '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Hence we see that Logistic Regression gives better model sensitivity, whereas predictive value(positive) is more for \n",
    "   Gaussian Naive Bayes. '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
